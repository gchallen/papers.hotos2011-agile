\section{Challenges}
\label{sec-challenges}

\input{./figures/transitiontable.tex}

To illustrate how a power-agile device might operate we imagine a phone
performing a background task interrupted by an interactive session.
Figure~\ref{figure-transitiongraph} shows how overall and per-component power
allocations change in response to the needs of the running applications. We
refer to this scenario throughout the rest of this section as we examine the
challenges inherent to power-agile computing. These are related to five roles
that the operating system has to to accomplish to operate power-agile
hardware: measuring~(\ref{subsec-measure}) and
predicting~(\ref{subsec-predict}) performance; along with
selecting~(\ref{subsec-select}, preparing~(\ref{subsec-prepare}) and
executing~(\ref{subsec-execute}) ensemble transitions. Throughout we
demonstrate how traditional problems of scheduling and resource allocation
are complicated by the changing nature of the underlying hardware.

\subsection{Measuring Efficiency}
\label{subsec-measure}

Determining performance differences between ensembles requires application
metrics weighting both power and performance. We believe that variants of the
energy-delay product used in circuit design~\cite{martin-et2} may be
appropriate for combining power and performance concerns. $E$ measures the
energy consumed during some time quantum and $\Delta$ measures a performance
characteristic of interest to the application: the time necessary to process
a block of data or respond to user input. Controlling the strength of the
performance component using an exponential, $\textrm{EDP} = E\Delta^n$,
allows applications to weight their prefererence for performance v.
efficiency. In our scenario, the interactive application uses
$E\Delta^2$---causing the system to activate high-energy and performance
ensembles---while the background task uses $E\sqrt{\Delta}$, causing the
system to remain in lower-power and performance ensembles. Application
metrics are likely to change over time as their needs change.

\subsection{Predicting Ensemble Performance}
\label{subsec-predict}

Given the size of the ensemble state space, predicting ensemble performance
is a key part of selection. Assuming an application with power-performance
preference $E\Delta^n$, both $E$ and $\Delta$ will vary across ensembles: $E$
with the cost and utilization of system components, and $\Delta$ with
performance. The most direct way to determine power-performance is to run the
application on many ensembles, but given the number of states and transition
cost this is infeasible online. However, offline experimentation could
produce binary annotations. Another approach is to have executables include
hints about performance characteristics important at various stages. Before
transmitting large amounts of information, a hint would alert the system to
the imminent need for a high-bandwidth radio. While hints require programmer
or compiler support, they may be portable across devices.

When running unannotated binaries, or mixtures of applications with
complicated performance dependencies, to system may need to estimate the
impact of ensemble changes before performing them. In some cases, the
currently running ensemble can be \textit{artificially} constrained to estimate how
performance might change after a component is disabled or enabled. For
example, when moving from \textbf{M2} to \textbf{M1} at t = 7 in the
scenario, the system might be concerned about the impact of this transition
on the usage of \textbf{S1}. If disabling the large memory chip causes usage
of stable storage to increase dramatically, then system will fail to achieve
the intended power reduction. To uncover a link between memory size and disk
usage, the operating system can artificially limit the amount of memory in
use by trimming pages from \textbf{M2}. It may do this in a smooth fashion
until it is using only roughly the same amount of the larger chip as the
smaller chip size, and then, assuming that no serious component relationships
have been uncovered, perform the transition. This strategy is generally more
applicable to transitions that attempt to trim power by disabling components,
but this is also precisely where it is most useful, as it allows the
operating system to discover relationships between component usage that might
neuter the transition.

\subsection{Selecting Component Ensembles}
\label{subsec-select}

Scheduling ensemble transitions relies on the capabilities already
presented---metrics for evaluating performance and estimating performance
across ensembles. When running only a single application the system can
respond directly to its hints, annotations, or estimated performance,
weighting possible performance improvements against the cost of transitioning
ensembles. Running multiple applications creates new challenges.

First, there is a question of assigning performance metrics to applications.
In our scenario the interactive application uses $E\Delta^2$ while the the
background task uses $E\sqrt{\Delta}$, but the background task would complete
faster if it were allowed to use the higher exponent. The goal is to assign
the most efficient metric to the application that produces acceptable
performance, and doing so is likely to require user feedback.

Choosing the correct ensemble for both applications is the next challenge. If
their performance are aligned, then an ensemble may exist that works well for
both. Applications differing in their performance requirements complicate the
process. We we have the energy to operate a combination of both ideal
ensembles, but this produces inefficiency as the set of distinct resources
needed by one application is idled while the other runs.

The simplest approach is to transition between the ideal ensembles while
increasing each applications time quanta sufficient to amortize the
transition cost. In many cases, however, we expect that this will lead to
unacceptable interactive performance. A second possible approach is to pick
an ensemble that produces acceptable---but not ideal---performance for
both applications, potentially weighted towards the application with higher
priority. Another option is to select an ensemble optimized for one
application while allocating resources within that ensemble in favor of the
other. For example, given one application that requires a high-speed disk and
another than needs a large memory chip, we can choose to use the large memory
chip and a slower disk allocating a large portion of the memory to a buffer
cache to improve performance for the I/O-bound application.

\subsection{Preparing Ensemble Transitions}
\label{subsec-prepare}

Because ensemble transitions are both important and costly, the operating
system should prepare the ground to minimize their overhead. Preparation is
particularly important in the memory and storage hierarchy, where the
location of data has a significant impact on component transitions.
Preparation also requires the system forecast future application demand and
ensemble dwell times.

Consider an example transition that activates a larger memory chip with
superior performance. If the system will be in that ensemble for a
significant length of time, all applications will benefit from having data
relocated from the smaller to the larger chip. This also allows the smaller
chip to be shut off to save power. However, if and when the device wants to
disable the larger memory chip in order to shift power toward some other
necessary component, the amount of data stored in the larger memory bank
creates a high overhead for this transition.

If the system predicts brief use of the larger memory bank, it may try
several strategies to reduce the eventual transition overhead. First, if the
transition is due to a particular application, it may continue to operate the
smaller chip for other applications while allocating new pages on the larger
component. Once the memory-hungry application is finished with these pages,
they can be discarded and the memory disabled without migrating data. Another
approach is to copy accessed pages on demand but mirror writes to both memory
banks to minimize the eventual transition cost. Assuming that the smaller
chip is never shut off---possible if consumes little power---the physical
address space may be configured to always mirror a portion to both chips when
the larger bank is active. The operating system may try to allocate memory
from the mirrored portion of the address space for pages that have long
expected lifetimes, are used by applications that prefer more power-efficient
states, or based on explicit application requests. These pages will benefit
from better performance when the larger bank is active while never requiring
migration.

\subsection{Executing Ensemble Transitions}
\label{subsec-execute}

Ensemble transitions tailor the device to application demands but require
potentially complex or energy-expensive component transitions. Below we
outline for each component class, the complexity and cost of transitions and
a brief description of how to perform one:

\begin{itemize}

\item \textbf{Processor:} Difficulty: \textit{high}, Cost: \textit{medium}.
Transitioning between processors, even ones with highly-compatible
instruction sets, requires migrating process state, correcting for processor
differences, and potentially reloading new process executables enabling or
disabling certain instructions.

\item \textbf{Memory:} Difficulty: \textit{medium}, Cost: \textit{high}.
Moving to a smaller chip requires migrating some pages to the new memory area
while flushing others to the backing store, along with kernel adjustments to
its own memory footprint. Transitioning to a larger chip requires migrating
data.

\item \textbf{Storage:} Difficulty: \textit{low}, Cost: \textit{low}.
Disabling requires writing out dirty buffers. Enabling will cause a
performance dip while caches fill.

\item \textbf{Radio:} Difficulty: \textit{medium}, Cost: \textit{medium}.
Disabling requires flushing any outstanding buffers, closing connections and
potentially coordinating with the receiver to move together to a new radio
technology. Enabling may require association---potentially costly, depending
on the protocol---and a delay while link parameters necessary for efficient
operation can be determined.

\end{itemize}
