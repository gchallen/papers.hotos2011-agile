\section{Challenges}
\label{sec-challenges}

\input{./figures/transitiontable.tex}

To illustrate how a power-agile device might operate we imagine a phone
performing a background task interrupted by an interactive session.
Figure~\ref{figure-transitiongraph} shows how overall and per-component power
allocations change to respond to the needs of the two applications. We refer
to this scenario throughout the rest of this section as we examine the
challenges inherent to power-agile computing. These are related to five roles
that the operating system plays while operating power-agile hardware:
measuring~(\ref{subsec-measure}) and predicting~(\ref{subsec-predict})
performance; and selecting~(\ref{subsec-select}),
preparing~(\ref{subsec-prepare}) and executing~(\ref{subsec-execute})
ensemble transitions. Throughout we demonstrate how traditional scheduling
and resource-allocation problems are complicated by the flexible nature of
the underlying hardware.

\subsection{Measuring Efficiency}
\label{subsec-measure}

Determining performance differences between ensembles requires application
metrics weighting both power and performance such as the energy-delay product
(EDP) commonly used in circuit design~\cite{martin-et2}. The EDP is defined
as $\textrm{EDP} = E\Delta$ where $E$ measures the energy consumed during
some time quantum and $\Delta$ measures a application-specific performance
characteristic such as the time necessary to process a block of data or
respond to user input. The system tries to minimize the EDP for each
application. Controlling the strength of the performance component using an
exponential---$\textrm{EDP} = E\Delta^n$---allows applications to weight
their preference for performance v. efficiency. In our scenario, the
interactive application uses $E\Delta^2$ causing the system to activate
high-performance ensembles; the background task uses $E\sqrt{\Delta}$,
causing the system to remain in lower-power states. Ensuring that
applications choose appropriate exponents and balancing between applications
at run-time are challenges inherent to this approach.

\subsection{Predicting Ensemble Performance}
\label{subsec-predict}

Given the size of the ensemble state space, predicting ensemble performance
is a key part of transitions. Assuming an application with preferred EDP
$E\Delta^n$, both $E$ and $\Delta$ will vary across ensembles: $E$ with the
cost and utilization of system components, and $\Delta$ with performance. The
direct way to determine power-performance is to run the application on many
ensembles, but given the number of states and transition cost this is
infeasible online. However, offline experimentation could produce binary
annotations. Another approach is to have executables include hints about
performance characteristics important to various stages. Before transmitting
a large amount of information, a hint would alert the system to the need for
a high-bandwidth radio. Hints have the advantage being portable across
devices, but they require programmer support and the system must ensure that
applications do not abuse them to gain unfair access to resources.

When running unannotated binaries or mixtures of applications with
performance dependencies, the system may need to estimate the impact of
ensemble changes before performing them. In some cases, the currently running
ensemble can be \textit{artificially} constrained to estimate how performance
might change after a component change. For example, when moving from
\textbf{M2} to \textbf{M1} at t = 7 in the scenario, the system might be
concerned about the impact of this transition on the usage of \textbf{S1}. If
disabling the large memory chip causes \textbf{S1} usage to increase
dramatically, the system will fail to achieve the intended power reduction.
To uncover a link between memory size and disk usage, the operating system
can artificially limit the amount of memory in use by trimming pages from
\textbf{M2}. It may do this in a smooth fashion until it is using only
roughly the same amount of the larger chip as the smaller chip size, and
then, assuming no serious component relationships have been uncovered,
initiate the transition. This strategy is more applicable to transitions that
attempt to trim power by disabling components, but this is also when it is
most useful, as it allows the operating system to discover relationships
between component usage that might neuter power reductions.

\subsection{Selecting Component Ensembles}
\label{subsec-select}

Scheduling ensemble transitions relies on the capabilities already
presented---metrics for evaluating performance and predicting performance
across ensembles. When running only a single application the system can
respond directly to its hints, annotations, or estimated performance,
weighting possible improvements in efficiency against the cost of
transitioning ensembles.

Running multiple applications creates new challenges. First, there is the
question of how to assign performance metrics to applications. In our
scenario the background task would complete faster if it were allowed to use
the higher exponent used by the interactive application. The goal is to
assign the most efficient metric to the application that produces acceptable
performance, and doing so is likely to require user feedback.

Choosing the correct ensemble for both applications is the next challenge. If
their performance requirements are aligned, then an ensemble may exist that
works well for both. Applications differing in their performance requirements
complicate the process. If the system has sufficient energy it may choose to
operate a combination of both ideal ensembles, but this produces inefficiency
as the set of distinct resources needed by one application is idled while the
other runs.

The simplest approach is to transition between the ideal ensembles while
increasing both application's time quanta sufficient to amortize the
transition cost. In many cases, however, we expect that this will lead to
unacceptable interactive performance. A second possible approach is to pick
an ensemble that produces acceptable---but not ideal---performance for both
applications, potentially weighted towards the application with higher
priority. Another option is to select an ensemble optimized for one
application while allocating resources within that ensemble in favor of the
other. For example, given one application that requires a high-speed disk and
another than needs a large memory chip, we can choose to use the large memory
chip and a slower disk allocating a large portion of the memory to a buffer
cache to improve performance for the I/O-bound application.

\subsection{Preparing Ensemble Transitions}
\label{subsec-prepare}

Because ensemble transitions are both important and costly, the operating
system should prepare the ground to minimize their overhead. Preparation is
particularly important in the memory and storage hierarchy, where the
location of data has a significant impact on component transitions.
Preparation also requires the system forecast future application demand and
ensemble dwell times.

Consider an example transition that activates a larger memory chip with
superior performance. If the system will be in that ensemble for a
significant length of time, all applications will benefit from having data
relocated from the smaller to the larger chip. This also allows the smaller
chip to be shut off to save power. However, if and when the device wants to
disable the larger memory chip in order to shift power toward some other
necessary component, the amount of data stored in the larger memory bank
creates a high overhead for this transition.

If the system predicts brief use of the larger memory bank, it may try
several strategies to reduce the eventual transition overhead. First, if the
transition is due to a particular application, it may continue to operate the
smaller chip for other applications while allocating new pages on the larger
component. Once the memory-hungry application is finished with these pages,
they can be discarded and the memory disabled without migrating data. Another
approach is to copy accessed pages on demand but mirror writes to both memory
banks to minimize the eventual transition cost. Assuming that the smaller
chip is never shut off---possible if consumes little power---the physical
address space may be configured to always mirror a portion to both chips when
the larger bank is active. The operating system may try to allocate memory
from the mirrored portion of the address space for pages that have long
expected lifetimes, are used by applications that prefer more power-efficient
states, or based on explicit application requests. These pages will benefit
from better performance when the larger bank is active while never requiring
migration.

\subsection{Executing Ensemble Transitions}
\label{subsec-execute}

Ensemble transitions tailor the device to application demands but may require
complex or expensive component transitions. The Advanced Configuration and
Power Interface (ACPI) specification~\cite{acpi-standard} standardizes
per-component and overall power states but does not consider component
transitions. Below we outline for each component class, the complexity and
cost of transitions and a brief description of how to perform one:

\begin{itemize}

\item \textbf{Processor:} Difficulty: \textit{high}, Cost: \textit{medium}.
Transitioning between processors, even ones with highly-compatible
instruction sets, requires migrating process state, correcting for processor
differences, and potentially reloading new process executables enabling or
disabling certain instructions.

\item \textbf{Memory:} Difficulty: \textit{medium}, Cost: \textit{high}.
Moving to a smaller chip requires migrating some pages to the new memory area
while flushing others to the backing store, along with kernel adjustments to
its own memory footprint. Transitioning to a larger chip requires migrating
data.

\item \textbf{Storage:} Difficulty: \textit{low}, Cost: \textit{low}.
Disabling requires writing out dirty buffers. Enabling will cause a
performance dip while caches fill.

\item \textbf{Radio:} Difficulty: \textit{medium}, Cost: \textit{medium}.
Disabling requires flushing any outstanding buffers, closing connections and
potentially coordinating with the receiver to move together to a new radio
technology. Enabling may require association---potentially costly, depending
on the protocol---and a delay while link parameters necessary for efficient
operation can be determined.

\end{itemize}
\vfill
